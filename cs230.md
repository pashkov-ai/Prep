https://www.youtube.com/watch?v=_NLHFoVNlbg&list=PLoROMvodv4rNRRGdS0rBbXOUGA0wjdh1X&index=1


# lecture 2
* contrastive self supervision (goal - learn embeddings)
* weakly supervised learning
* ImageBIND https://arxiv.org/abs/2305.05665 https://imagebind.metademolab.com/

# lecture 3
* ML lifecycle(data,modeling,monitoring)
* having train nad test data from the same distribution isnot that important anymore (with large models)
* inexpensive models first, expensive modelnext - for cost optimization
* data/concept drift
* monitoring (for things that could go wrong)

# lecture 4
## adversarial robustness
* * intriguing properties of nn https://arxiv.org/abs/1312.6199
* *  Explaining and Harnessing Adversarial Examples https://arxiv.org/abs/1412.6572
* * Adversarial examples in the physical world https://arxiv.org/abs/1607.02533
* * Fooling automated surveillance cameras: adversarial patches to attack person detection
* * 14-18 adversarial pixels
* * 18-21 data poisoning
* * 22-25 prompt injections
* attacks on instructions, context, retrival pipelines, 
* define loss function https://youtu.be/aWlRtOlacYM?list=PLoROMvodv4rNRRGdS0rBbXOUGA0wjdh1X&t=751
* fast gradient sign method (oneshot) https://www.tensorflow.org/tutorials/generative/adversarial_fgsm
* Adversarial Examples in Modern Machine Learning: A Review https://arxiv.org/abs/1911.05268
* Constitutional AI: Harmlessness from AI Feedback https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback
* Detecting and Rejecting Adversarial Examples Robustly https://arxiv.org/abs/1704.00103
* Adversarial Logit Pairing https://arxiv.org/abs/1803.06373
* input sanitization, output filtering, training with adversarial examples, adversarial training, red teaming
* Prompt Injection attack against LLM-integrated Applications https://arxiv.org/abs/2306.05499
* An Early Categorization of Prompt Injection Attacks on Large Language Modelshttps://arxiv.org/abs/2402.00898
## generative models
* training principle: #images >> #params
* selfsupervised learning, goal - generate content from original distribution (unlike contrastive learning)
* ex. detect humans on drone imagery, remove them and inpaint
* Generative Adversarial Networks (GANs)
* * generator-discriminator
* * Problem: mode collapse (G foolsD but not generating real images)
* * Problem: vanishing gradients, difficult to converge
* * operation codes (same as encoding math)
* Diffusion models
* * Diffusion Models Beat GANs on Image Synthesis https://arxiv.org/abs/2105.05233
* * core idea: progressively adds noise and learns to reverse this process to generate samples
* * Sampling: model predicts cumulative noise (from noise input): noise_input - pred_noise -> image (many iterations)
* * Denoising Diffusion Probabilistic Models (33k!)https://arxiv.org/abs/2006.11239
* * High-Resolution Image Synthesis with Latent Diffusion Models (28k!) https://arxiv.org/abs/2112.10752
* ??? todo: Stable Diffusion???
* Imagen, Veo3, Make-a-video, Dall-E, Sora 2

# lecture 5
## Deep Reinforcement Learning
* todo papers on 1st slide
* applications: ads, robotics, games
* agent (action) - environment (statechange) -> (observation, reward) interaction
* Transition: St -> At -> (Ot, Rt) -> St+1
* Discounted return
* Bellman equation (optimality equation)
* Policy
### Deep Q-Learning
* Q-function instead of Q-table
* input preprocessing
* Replay memory (experience storage, sampled during training)
* exploration vs exploitation
* Human-level control through deep reinforcement learning (paper)
* The coming wave(book)
* Generative Adversarial Imitation Learning https://arxiv.org/abs/1606.03476
* Unifying Count-Based Exploration and Intrinsic Motivation https://arxiv.org/abs/1606.01868
### Proximal policy Optimization (PPO) https://arxiv.org/abs/1707.06347
* https://openai.com/index/competitive-self-play/
* https://openai.com/index/openai-five/
* https://deepmind.google/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/
### reinforcement learning from human feedback RLHF
* Training language models to follow instructions with human feedback https://arxiv.org/abs/2203.02155
* SFT
* critic

# lecture 7???

# lecture 8
## prompting
* Chain-of-Thought Prompting Elicits Reasoning in Large Language Models https://arxiv.org/abs/2201.11903
* https://github.com/f/awesome-chatgpt-prompts
* zero-shot vs few-shot ptompting
* https://platform.claude.com/docs/en/resources/prompt-library/library
* chain of thoughts
* chain complex prompts
* https://github.com/promptfoo/promptfoo
* LLM Judges

## Fine tuning (better to avoid)
* https://rosslazer.com/posts/fine-tuning/

## Retrieval Augmented Generation (RAG)
* https://arxiv.org/abs/2206.01226
* embedding docs (triplet loss, etc??)
* vector DB
* Retrieval-Augmented Generation for Large Language Models: A Survey https://arxiv.org/abs/2312.10997

## Agentic AI
* MCP https://modelcontextprotocol.io/docs/getting-started/intro
* LLM traces
* agentfoundry
* mixture of experts

## Interpretable AI
* Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps https://arxiv.org/abs/1312.6034
* CNN:
* saliency maps
* occlusion sensitivity
* class activation maps
* gradient ascent
* data search
* deconvolution

* scaling curve alignment
* router entropy & expert load (MoE)
* gradient norms (per layer)
* LR schedule curve 
* checkpoint-to-checkpoint eval
* token distribution drift reports
* tokenizer stats
* batch level anomalies